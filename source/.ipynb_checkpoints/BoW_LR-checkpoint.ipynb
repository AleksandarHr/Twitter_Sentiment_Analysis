{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from classes import PreProcessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = pd.read_fwf('../Data/train_neg.txt', header=None, names=['tweet'])\n",
    "train_pos = pd.read_fwf('../Data/train_pos.txt', header=None, names=['tweet'])\n",
    "test = pd.read_csv('../Data/test_data.txt', sep='\\n', header=None, names=['tweet'])\n",
    "test['tweet-id'] = test.tweet.apply(lambda x: x.split(',')[0])\n",
    "test['tweet'] = test.tweet.apply(lambda x: ' '.join(x.split(',')[1:]))\n",
    "test = test.set_index('tweet-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessing()\n",
    "# Clean the tweets\n",
    "train_pos_clean = train_pos.copy()\n",
    "train_neg_clean = train_neg.copy()\n",
    "test_clean = test.copy()\n",
    "train_pos_clean['tweet'] = train_pos_clean.tweet.apply(lambda x: preprocessor.clean(x))\n",
    "train_neg_clean['tweet'] = train_neg_clean.tweet.apply(lambda x: preprocessor.clean(x))\n",
    "test_clean['tweet'] = test_clean.tweet.apply(lambda x: preprocessor.clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.read_fwf('../Data/vocab_cut.txt', header=None, names=['tweet'])\n",
    "# Convert sentences to list of words\n",
    "train_pos_clean['tweet'] = train_pos_clean.tweet.apply(lambda x: x.split(' '))\n",
    "train_neg_clean['tweet'] = train_neg_clean.tweet.apply(lambda x: x.split(' '))\n",
    "\n",
    "# Save tweet's id\n",
    "train_pos_clean['tweet-id'] = train_pos_clean.index\n",
    "train_neg_clean['tweet-id'] = train_neg_clean.index\n",
    "\n",
    "train_pos_clean = train_pos_clean.explode('tweet')\n",
    "train_neg_clean = train_neg_clean.explode('tweet')\n",
    "\n",
    "# Inner join with vocabulary to filter words. \n",
    "train_pos_final = pd.DataFrame(train_pos_clean.merge(vocabulary, how='inner').groupby('tweet-id')['tweet'].apply(list))\n",
    "train_neg_final = pd.DataFrame(train_neg_clean.merge(vocabulary, how='inner').groupby('tweet-id')['tweet'].apply(list))\n",
    "\n",
    "train_neg_final['sentiment'] = 0\n",
    "train_pos_final['sentiment'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_pos_final[['tweet', 'sentiment']], train_neg_final[['tweet', 'sentiment']]], ignore_index=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = train_data['tweet']\n",
    "y = train_data['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y, test_size = 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " '<user>',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'duno',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'justin',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'read',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'or',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " 'not',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'only',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'god',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'knows',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'about',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'but',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'you',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'will',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " 'follow',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(X):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    print ('Creating bag of words...')\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.  \n",
    "    \n",
    "    # In this example features may be single words or two consecutive words\n",
    "    # (as shown by ngram_range = 1,2)\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 ngram_range = (1,2), \\\n",
    "                                 max_features = 10000\n",
    "                                ) \n",
    "\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of \n",
    "    # strings. The output is a sparse array\n",
    "    train_data_features = vectorizer.fit_transform(X)\n",
    "    \n",
    "    # Convert to a NumPy array for easy of handling\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    \n",
    "    # tfidf transform\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_features = tfidf.fit_transform(train_data_features).toarray()\n",
    "\n",
    "    # Get words in the vocabulary\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "   \n",
    "    return vectorizer, vocab, train_data_features, tfidf_features, tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bag of words...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-e9751312c5a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_bag_of_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-89345a610d04>\u001b[0m in \u001b[0;36mcreate_bag_of_words\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# into feature vectors. The input to fit_transform should be a list of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# strings. The output is a sparse array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtrain_data_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Convert to a NumPy array for easy of handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\envs\\project2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1032\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\envs\\project2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\envs\\project2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    326\u001b[0m                                                tokenize)\n\u001b[0;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 328\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\envs\\project2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectorizer, vocab, train_data_features, tfidf_features, tfidf = create_bag_of_words(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "project2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
