{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove\n",
    "This notebook presents the methods that were used to get from cleaned tweets to sentiment predictions. The objective of this notebook is to assess the performance of the [Glove](https://nlp.stanford.edu/projects/glove/) word representation. Others notebooks were made with different word representations, the focus is more on evalutating the performance under a fix setup rather than getting the absolute best performance. We therefore used the [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) which is a relatively light, interpretable and well suited algorithm for a binary classification task. The library [scikit-learn](https://scikit-learn.org/stable/) will be used extensively throughout this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the clean data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clean data\n",
    "train_data = pd.read_csv('../Data/train_small.txt')\n",
    "\n",
    "# Shuffle the data\n",
    "train_data = train_data.sample(train_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Glove\n",
    "The main drawbacks from the [Bag-of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) representations are as follows:\n",
    "\n",
    "- The dimension of the embedding space depends of the number of distinct words in the corpus.\n",
    "- The embedding representation of a given document is sparse.\n",
    "- The embedding representation of a given document does not take into account the words ordering.\n",
    "- Sementically identical document using different set of words (synonyms) might/will have a cosine similarity of zero.\n",
    "\n",
    "The [Glove](https://nlp.stanford.edu/projects/glove/) word representation aims to solve the previously mentionned flaws by encoding a word with respect to the context in which it appears. A more detailed explanation can be found in the report. The Glove embeddings used in this notebook were trained on a set of 2 billions tweets and is therefore particularly suited for our goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained embedding with a specified dimension here 200\n",
    "words_embedding = pd.read_table('../Data/glove.twitter.27B.200d.txt', sep=\" \", index_col=0, \n",
    "                                header=None, quoting=csv.QUOTE_NONE)\n",
    "words_embedding = words_embedding[~words_embedding.index.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pre-trained embedding loaded, we can remove from the embedding matrix the rows that won't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary from the train set of tweets\n",
    "vocabulary = pd.DataFrame(train_data['tweet'].apply(lambda x: x.split(' ')).explode()).drop_duplicates()\n",
    "vocabulary = vocabulary = vocabulary.rename(columns={'index': 'word'})\n",
    "\n",
    "# Removing unused words from pretrained embeddings\n",
    "words_embedding = words_embedding.merge(vocabulary, how='inner', left_on=words_embedding.index, right_on='tweet')\n",
    "words_embedding = words_embedding.rename(columns={'tweet': 'word'})\n",
    "words_embedding = words_embedding.set_index('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dictionnary out of the dataframe for faster access\n",
    "embedding_dict = dict(zip(words_embedding.index, words_embedding[words_embedding.columns].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the vocabulary cleaned from unlikely words, the tweets should also be treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tweets(tweet, vocabulary):\n",
    "    \"\"\"\n",
    "    remove any word from the tweet which does not appear in the dictionary\n",
    "    @param tweet: single tweet as a string\n",
    "    @param vocabulary: vocabulary as a set\n",
    "    @return: single cleaned tweet\n",
    "    \"\"\"\n",
    "    tweet = list(filter(lambda word: word in vocabulary, tweet.split(' ')))\n",
    "    tweet = ' '.join(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary as a set for O(1) access\n",
    "vocabulary_set = set(embedding_dict.keys())\n",
    "\n",
    "# Remove non-vocabulary words from tweets \n",
    "train_data['tweet'] = train_data['tweet'].apply(lambda tweet: filter_tweets(tweet, vocabulary_set))\n",
    "train_data = train_data[train_data['tweet'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data and embedding matrix are ready to be used we can proceed to the actual encoding of the tweets. Since a typical tweet contains more than a single word and the embedding at hand is at a word level we chose to encode a tweet by averaging its words' embeddings. Note that by doing so the ordering of the terms in a document is lost, a finer approach will be used when dealing with neural networks which can use sequences as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_word_embedding(tweet, dictionary):\n",
    "    \"\"\"\n",
    "    computes the embedding of a tweet by averaging the embedding of its words\n",
    "    @param tweet: single tweet as a string\n",
    "    @param dictionary: dictionary mapping words to their embedding representation\n",
    "    @return: mean embedding of the tweet's words\n",
    "    \"\"\"\n",
    "    mean_embedding = np.mean([dictionary[word] for word in tweet.split(' ')], axis=0)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mean_word_embedding\n",
    "train_data['embedding'] = train_data['tweet'].apply(lambda x: mean_word_embedding(x, embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['embedding']\n",
    "X_train = np.stack(X_train.to_numpy())\n",
    "y_train = train_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the tweets have numerical representation, they can be fed to the Logistic Regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "accuracy = cross_val_score(logistic, X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(), accuracy.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an accuracy slightly lower than with the TF-IDF embedding, nonetheless the number of dimensions of the embedding space of the Glove embedding is 25 times lower, which directly transfers to a 25 times lower number of paramters in the model when using Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
