{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "This notebook presents the methods that were used to clean the raw tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import itertools\n",
    "from helpers import *\n",
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "neg = pd.read_fwf('../Data/train_neg.txt', header=None, names=['tweet'])\n",
    "pos = pd.read_fwf('../Data/train_pos.txt', header=None, names=['tweet'])\n",
    "test = pd.read_csv('../Data/test_data.txt', sep='\\n', header=None, names=['tweet'])\n",
    "test['tweet-id'] = test.tweet.apply(lambda x: x.split(',')[0])\n",
    "test['tweet'] = test.tweet.apply(lambda x: ' '.join(x.split(',')[1:]))\n",
    "test = test.set_index('tweet-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i dunno justin read my mention or not ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic is so dumb , i won't even c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put casper in a box ! \" looved t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thanks sir &gt; &gt; don't trip lil ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother tmr is the bestest birthda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  <user> i dunno justin read my mention or not ....\n",
       "1  because your logic is so dumb , i won't even c...\n",
       "2  \" <user> just put casper in a box ! \" looved t...\n",
       "3  <user> <user> thanks sir > > don't trip lil ma...\n",
       "4  visiting my brother tmr is the bestest birthda..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can observe in the above set of tweets, the data at hand is not particularly cleaned which can lead to under performing algorithm. Indeed it is important to get rid of the noise in the tweets to prevent our future algorithm to overfit on it. Furthermore, cleaning the data can also help reducing the number of word in the vocabulary by merging semantically similar words together, for instance 'I'm' and 'Im'. This is a desirable property when using either [Bag-of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) or [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) as word representation as it can significantly decrease the dimension of the embedding space. When using the [Glove](https://nlp.stanford.edu/projects/glove/) representation this property is again desirable as it increases the chance of a given word to have a pre-trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained embedding\n",
    "words_embedding = pd.read_table('../Data/glove.twitter.27B.25d.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "words_embedding = words_embedding[~words_embedding.index.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(tweet):\n",
    "    \"\"\"\n",
    "    @param tweet: single tweet as a string\n",
    "    @return: single cleaned tweet as a string\n",
    "    \"\"\"\n",
    "    # Instansiate tokenizer\n",
    "    tokenizer = TweetTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Removing HTML characters\n",
    "    tweet = BeautifulSoup(tweet).get_text()\n",
    "\n",
    "    # As the character \"'\" will be part of the contractions it must be kept\n",
    "    tweet = tweet.replace('\\x92', \"'\")\n",
    "    tweet = tweet.replace(\"â€™\", \"'\")\n",
    "\n",
    "    # Remove words that start with '#' or '@' \n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "\n",
    "    # Remove web addresses\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "    # Removal of punctuation\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\(\\)\\d\\\"\\_\\<\\>\\+\\@\\/]\", \" \", tweet).split())\n",
    "\n",
    "    # contractions source:\n",
    "    contractions = load_dict_contractions()\n",
    "    words = tokenizer.tokenize(tweet)\n",
    "    words = [lemmatizer.lemmatize(word, 'v') for word in words]\n",
    "    words = list(filter(lambda word: len(word) > 1, words))\n",
    "    reformed = [contractions[word] if word in contractions else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first step of the cleaning consists of removing undesired characters, applying some lemmatization on the words, removing single letter words and finally correcting some of spelling errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweets\n",
    "neg['tweet'] = neg['tweet'].apply(lambda tweet: cleaning(tweet))\n",
    "pos['tweet'] = pos['tweet'].apply(lambda tweet: cleaning(tweet))\n",
    "test['tweet'] = test['tweet'].apply(lambda tweet: cleaning(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words appear more that others. The ones that appear the most in a language are called stop-words and are often removed from the corpus as they appear in almost every single document and thus do not bear a lot of predictive power. Nonetheless we chose to keep them. Indeed words such as 'not' or 'no' are considered as stop-words but could potentially be helpful in the case of a sentiment analysis task. On the other hand words which appear very few times might have a less significant impact and are also less likely to be in a pre-trained vocabulary. A closer look at the words appearing only once shows that some of those words could get mapped to more common words by a finer pre-processing step. We chose to remove from the document any term used less than a given  threshold. The threshold should be adapted to the size of the corpus, ideally it should rather be a probability of observing a word as it would mitigate the problem of varying size corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute vocabulary i.e. all words appearing at least once in either of the cleaned file\n",
    "vocabulary = pd.concat([neg['tweet'].apply(lambda x: x.split(' ')).explode(), \n",
    "                        pos['tweet'].apply(lambda x: x.split(' ')).explode(),\n",
    "                        test['tweet'].apply(lambda x: x.split(' ')).explode()])\n",
    "vocabulary = pd.DataFrame(vocabulary)\n",
    "\n",
    "# Computes the appearing frequency of each word in the vocabulary\n",
    "words_frequencies = pd.DataFrame(vocabulary['tweet'].value_counts())\n",
    "words_frequencies.rename(columns = {'tweet':'count'}, inplace = True) \n",
    "\n",
    "# Keep only words that appear at least a given amount of times here 2\n",
    "vocabulary = words_frequencies[words_frequencies['count'] >= 2].reset_index(level=0)[['index']]\n",
    "vocabulary = vocabulary.rename(columns={'index': 'word'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the vocabulary cleaned from unlikely words, the tweets should also be treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tweets(tweet, vocabulary):\n",
    "    \"\"\"\n",
    "    remove any word from the tweet which does not appear in the dictionary\n",
    "    @param tweet: single tweet as a string\n",
    "    @param vocabulary: vocabulary as a set\n",
    "    @return: single cleaned tweet\n",
    "    \"\"\"\n",
    "    tweet = list(filter(lambda word: word in vocabulary, tweet.split(' ')))\n",
    "    tweet = ' '.join(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary as a set for O(1) access\n",
    "vocabulary_set = set(vocabulary['word'].to_list())\n",
    "\n",
    "# Remove non-vocabulary words from tweets \n",
    "test['tweet'] = test['tweet'].apply(lambda tweet: filter_tweets(tweet, vocabulary_set))\n",
    "pos['tweet'] = pos['tweet'].apply(lambda tweet: filter_tweets(tweet, vocabulary_set))\n",
    "neg['tweet'] = neg['tweet'].apply(lambda tweet: filter_tweets(tweet, vocabulary_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet    12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking for empty tweets\n",
    "test[test['tweet'] == ''].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the previous step some of the tweets end up being empty. With the large set of tweets and a threshold of five apparition only three tweets in the test set end up being empty. The empty tweets are treated differently if they appear in the train set than in the test set:\n",
    "   - Train set: The empty tweet is simply dropped.\n",
    "   - Test set: The empty tweet is filled with the tag `<empty>` which is not in the dictionnary and should have a positive/negative sentiment associated to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['tweet'] == ''] = '<empty>'\n",
    "vocabulary = vocabulary.append({'word': '<empty>'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels\n",
    "pos['label'] = 1\n",
    "neg['label'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When usig the Glove embedding one can also get rid of words which do not appear in the corpus as they won't be of any use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unused words from pretrained embeddings\n",
    "words_embedding = words_embedding.merge(vocabulary, how='inner', left_on=words_embedding.index, right_on='word')\n",
    "words_embedding = words_embedding.set_index('word')\n",
    "\n",
    "# Making a dictionnary out of the dataframe for faster access\n",
    "embedding_dict = dict(zip(words_embedding.index, words_embedding[words_embedding.columns].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12879352718636955"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the percentage of unmatched words\n",
    "len(set(vocabulary['word']) - set(embedding_dict.keys())) / len(set(vocabulary['word']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the preprocessing step $12 \\%$ of the terms in the corpus do not have a pre-trained representation. This number is low but not insignificant we thus chose to add an entry for each of them in the embedding matrix. The latter can then be fed to a [nn.Embedding](https://pytorch.org/docs/master/nn.html#torch.nn.Embedding) layer which will/can be further trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addig missing entries to the embedding matrix\n",
    "embedding_matrix, word_indexer = build_embedding_matrix(embedding_dim=25, \n",
    "                                                        glove=embedding_dict, \n",
    "                                                        vocabulary=vocabulary['word'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we can now split the data in a train, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the different sets\n",
    "train_ratio = 0.75\n",
    "valid_ratio = 0.125\n",
    "train_stop = int(train_ratio * neg.shape[0])\n",
    "valid_stop = int((train_ratio + valid_ratio) * neg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data in train, validation and test\n",
    "train_data = pd.concat([pos[['tweet', 'label']].iloc[:train_stop], \n",
    "                        neg[['tweet', 'label']].iloc[:train_stop]], \n",
    "                       ignore_index=True)\n",
    "valid_data = pd.concat([pos[['tweet', 'label']].iloc[train_stop: valid_stop], \n",
    "                        neg[['tweet', 'label']].iloc[train_stop: valid_stop]],\n",
    "                       ignore_index=True)\n",
    "test_data = pd.concat([pos[['tweet', 'label']].iloc[valid_stop:],\n",
    "                       neg[['tweet', 'label']].iloc[valid_stop:]], \n",
    "                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empty tweets can now be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data['tweet'] != '']\n",
    "valid_data = valid_data[valid_data['tweet'] != '']\n",
    "test_data = test_data[test_data['tweet'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the clean data set ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../Data/train_small.txt', index=False)\n",
    "valid_data.to_csv('../Data/valid_small.txt', index=False)\n",
    "test_data.to_csv('../Data/test_small.txt', index=False)\n",
    "test.to_csv('../Data/test_online_small.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Data/embedding_small', embedding_matrix)\n",
    "np.save('../Data/embedding_indexer_small', word_indexer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
