{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "This notebook presents the methods that were used to get from cleaned tweets to sentiment predictions. The objective of this notebook is to assess the performance of the  [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) word representation. Others notebooks were made with different word representations, the focus is more on evalutating the performance under a fix setup rather than getting the absolute best performance. We therefore used the [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) which is a relatively light, interpretable and well suited algorithm for a binary classification task. The library [scikit-learn](https://scikit-learn.org/stable/) will be used extensively throughout this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the clean data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clean train data\n",
    "train_data = pd.read_csv('../Data/train_small.txt')\n",
    "\n",
    "# Shuffle the data\n",
    "train_data = train_data.sample(train_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF \n",
    "TF-IDF improves the [Bag-of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation by taking into account the frequency of a given inside the whole corpus. In other words (no pun intended), it gives more weights to terms that appear seldomly. A more detailed explanation can be found in the report. One of the main drawback of this representation, aside from its sparsity, is that the embedding dimension grows with the number of different words in the corpus. This problem mitigated by artificially fixing the number of features. The latter trick becomes increasingly important if one decide to not only represent single words but also [n-grams](https://en.wikipedia.org/wiki/N-gram) as they can potentially improve the predictive power of a given model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the features extractor\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 1))\n",
    "\n",
    "# Get embedding representation of the tweets\n",
    "X_train = vectorizer.fit_transform(train_data['tweet']).toarray()\n",
    "y_train = train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149915, 5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the dimension is [#tweets, max_features]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the tweets have numerical representation, they can be fed to the Logistic Regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "accuracy = cross_val_score(logistic, X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(), accuracy.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with the raw data\n",
    "We are now interested to measure the impact of the pre-processing step on the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "neg = pd.read_fwf('../Data/train_neg.txt', header=None, names=['tweet'])\n",
    "pos = pd.read_fwf('../Data/train_pos.txt', header=None, names=['tweet'])\n",
    "\n",
    "# Add label\n",
    "neg['label'] = 0\n",
    "pos['label'] = 1\n",
    "\n",
    "# Concatenate the pos and neg dataframes\n",
    "train_data_raw = pd.concat([pos, neg])\n",
    "\n",
    "# Using the same number of samples as the cleaned data for a fair comparison\n",
    "train_data_raw = train_data_raw.sample(train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the features extractor\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 1))\n",
    "\n",
    "# Get embedding representation of the tweets\n",
    "X_train_raw = vectorizer.fit_transform(train_data_raw['tweet']).toarray()\n",
    "y_train_raw = train_data_raw['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "accuracy = cross_val_score(logistic, X_train_raw, y_train_raw, cv=3, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(), accuracy.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy with and without the pre-processing step is exactly the same, nevertheless these two numbers are not sufficient to assess the performance of the pre-processing. Indeed, the latter also aimed at reducing the number of words in the corpus such that the missing words could later be trained using a [nn.Embedding](https://pytorch.org/docs/master/nn.html#torch.nn.Embedding) layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.352505582385626"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing the size of the vocabulary\n",
    "clean_vocabulary = pd.DataFrame(train_data['tweet'].apply(lambda x: x.split(' ')).explode()).drop_duplicates()\n",
    "raw_vocabulary = pd.DataFrame(train_data_raw['tweet'].apply(lambda x: x.split(' ')).explode()).drop_duplicates()\n",
    "clean_vocabulary.shape[0] / raw_vocabulary.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of words in the clean vocabulary is significcantly lower than in the raw one. As the final model will be trained on the large set of tweets it is more valuable to compare the reduction in the vocabulary with this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full raw data\n",
    "neg_full = pd.read_fwf('../Data/train_neg_full.txt', header=None, names=['tweet'])\n",
    "pos_full = pd.read_fwf('../Data/train_pos_full.txt', header=None, names=['tweet'])\n",
    "\n",
    "# Concatenate the data\n",
    "train_data_raw_full = pd.concat([pos_full, neg_full])\n",
    "\n",
    "# Load the full clean data\n",
    "train_data_full = pd.read_csv('../Data/train_full.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12322306856745761"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_vocabulary_full = pd.DataFrame(train_data_raw_full['tweet'].apply(lambda x: x.split(' ')).\\\n",
    "                                   explode()).drop_duplicates()\n",
    "clean_vocabulary_full = pd.DataFrame(train_data_full['tweet'].apply(lambda x: x.split(' ')).\\\n",
    "                                     explode()).drop_duplicates()\n",
    "clean_vocabulary_full.shape[0] / raw_vocabulary_full.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall size of the vocabulary is significantly reduced."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
