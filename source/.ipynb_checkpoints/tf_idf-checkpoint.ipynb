{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "This notebook presents one of the methods that were used for word vectorization using the pre-processed (\"clean\") tweets. The objective of this notebook is to assess the performance of the [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) word representation. Others notebooks were made with different word representations. The focus lies more in evaluating the performance under a fixed setup rather than getting the absolute best performance. We therefore used the [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression), which is a relatively light, interpretable, and well suited algorithm for a binary classification task. The library [scikit-learn](https://scikit-learn.org/stable/) will be used extensively throughout this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the clean data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clean train data\n",
    "train_data = pd.read_csv('../Data/train_small.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF \n",
    "TF-IDF improves the [Bag-of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation by taking into account the frequency of a given inside the whole corpus. In other words, it gives more weights to terms that appear seldomly. A more detailed explanation can be found in the report. One of the main drawbacks of this representation, aside from its sparsity, is that the embedding dimension grows with the number of different words in the corpus. This problem mitigated by artificially fixing the number of features. The latter trick becomes increasingly important if one decide to not only represent single words but also [n-grams](https://en.wikipedia.org/wiki/N-gram) as they can potentially improve the predictive power of a given model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the features extractor\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 1))\n",
    "\n",
    "# Get embedding representation of the tweets\n",
    "X_train = vectorizer.fit_transform(train_data['tweet']).toarray()\n",
    "y_train = train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149915, 1500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the dimension is [#tweets, max_features]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the tweets have numerical representation, they can be fed to the Logistic Regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "accuracy = cross_val_score(logistic, X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(), accuracy.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
